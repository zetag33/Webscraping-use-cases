{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**What's web commercial web scraping and why it is valuable?**"
      ],
      "metadata": {
        "id": "K7VBfquIaHNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping definition:\n",
        "https://en.wikipedia.org/wiki/Web_scraping\n",
        "\n",
        "An important part of web scraping is concentrated into web scraping of commercial webpages in order to collect data for market studies such as: e-commerce, real state or service offering.\n",
        "\n",
        "The study of this data such as prices, locations, quantities, providers, etc. has lots of commercial value and can bring a lot of business insights specially when real-time data and analytics are needed.\n",
        "\n",
        "Web-scraping bots combined with data storage and real-time analytics can provide your business with automated insights on the prices, quantities, locations and overall state of the market your business is competing in.\n",
        "\n",
        "Imagine having a completely automated tool that runs 24/7 collecting, storing and analyzing valuable data for your business.\n",
        "\n",
        "Tools such as Python, Cloud Computing and NoSQL databases have provided this field with huge potential."
      ],
      "metadata": {
        "id": "p5Cm9ZCkYQnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What's a commercial website , what's a catalog of offers and how can we extract value from its structure?**"
      ],
      "metadata": {
        "id": "P4lff_VRb5an"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A commercial website definition is as simple as it's own name:\n",
        "\n",
        "It's a website(https://en.wikipedia.org/wiki/Website) dedicated to e-commerce(https://en.wikipedia.org/wiki/E-commerce) or simply dedicated to traditional commerce and trade while hosting it's offers also on the Internet as a form of reaching more clients.\n",
        "\n",
        "As an example:\n",
        "\n",
        "[Amazon Website](https://www.amazon.es/)\n",
        "\n",
        "There is a characteristic shared between commercial webpages, all of them have a catalog where they group their resources.\n",
        "\n",
        "Inside this catalog we have all the urls that redirect to all the offers for each of the categories.\n",
        "\n",
        "As an example, in the amazon website url provided above at the top left there is a displayable with the name of \"All\" if we click in, we are shown a displayable with the different categories of products provided by Amazon, each category has its own url.\n",
        "\n",
        "Once we access an specific category, we will be shown a list of products and its characteristics, the bot would collect the desired data and then pass to the next page of products of the category or simply scroll down until there are no more products depending on the structure of the website.\n",
        "\n",
        "This would be done for each page of each category until there are no more products.\n",
        "\n",
        "When we combine different sources of data(websites) we are able to get an overall real-time vision of a concrete market that could not be achieved otherwise.\n",
        "\n",
        "This repetitive structure of commercial websites allows us to a create a base of code that remains the same for most cases.\n",
        "\n",
        "We know that we are going to get all the url's for each category and then access each category and surf through the catalog, this will be needed to do for all commercial websites.\n",
        "\n",
        "So given this structure, the only aspect we have to change between websites would be the name of the elements inside the HTML, where the desired data is located, all the other methodology and code can remain unchanged.\n",
        "\n",
        "This allows great versatility and low effort in scraping lots of different domains. Providing us with a huge chunk of real-time and real-market data to store and analyse.\n",
        "\n",
        "This tutorial will only be focused in the methodology of collection and the fixed structure to scrap a commercial website."
      ],
      "metadata": {
        "id": "imcPqm2BcgCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Use-case and creation of the process of collection**"
      ],
      "metadata": {
        "id": "ayf85ttNniC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Particular use-case 1**"
      ],
      "metadata": {
        "id": "ZqIf435YAhqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our example we are going to scrap Fotocasa a spanish portal containing data about real state in Spain. It's basically a portal where owners advertise the properties they are selling.\n",
        "\n",
        "In this case our targeted is data belonging to properties on sale, not rent.\n",
        "\n",
        "So the first step of any scrap process should be what's my starting point.\n",
        "\n",
        "The starting point should always be an url that contains the catalog of the products/services we are interested in.\n",
        "\n",
        "It might be a url completely dedicated to be the catalog or it could be any url of the domain were there is a displayable accessing the catalog.\n",
        "\n",
        "In our example all url's of the subdomain dedicated to selling properties offer a displayable with the catalog, we start at the main one, could be any other pertaining to the subdomain.\n",
        "\n",
        "Firstly we'll access the url: https://www.fotocasa.es/es/comprar/viviendas/espana/todas-las-zonas/l\n",
        "\n",
        "If we access this url, at the top left we can see a displayable named 'Provincia' meaning State/Province in spanish, if we click on it, it displays a list of all the provinces in Spain, each province has a href(HTML element) containing the url that contains the offers for that province.\n",
        "\n",
        "What our bot is going to do is access that url, click on the displayable, scroll down until the bottom of the displayable so all provinces are displayed, capture the url of each of the provinces.\n",
        "\n",
        "Once arrived here, we got the first step of any commercial webscraping bot, we have got all the root urls that we are going to scrap.\n",
        "\n",
        "As an example: https://www.fotocasa.es/es/comprar/viviendas/asturias-provincia/todas-las-zonas/l\n",
        "\n",
        "Contains the offers for one specific province.\n",
        "\n",
        "We are going to access it, scroll down the page and get all the data of each property offered, once we are at the bottom and there are no more advertisements we are going to capture the next url of the catalog pertaining to that province(equivalent to pressing next button to display next page) and do the same over and over until there are no more advertisements for the given province. We'll do this process for each province.\n",
        "\n",
        "We are capturing:\n",
        "\n",
        "- Title of the advertisement.\n",
        "- Price\n",
        "- Price reduction from initial price.\n",
        "- Rooms\n",
        "- Bathrooms\n",
        "- Meters\n",
        "- Province"
      ],
      "metadata": {
        "id": "5BFFy-ceApwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Technical process of collection**"
      ],
      "metadata": {
        "id": "duiE-YvuEkrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our bot will be coded in Python.\n",
        "\n",
        "I am specifically using Selenium a JavaScript rendering framework that allows us to programatically interact with webpages as we were a human using a browser.\n",
        "\n",
        "I am also using a proxy provider for technical reasons explained in the code section.\n",
        "\n",
        "You'll need to configure your proxy provider so that you can connect to its services using Python. In my case I whitelisted my IP in the proxy provider management tools so I can access my proxies without providing user or password,\n",
        "this may change between providers.\n",
        "\n",
        "In order for this processes to be interesting they must scrap thousands and milions of urls, this implies that the bigger your project is bigger will be your computing necesities.\n",
        "\n",
        "You'll need multiprocessing in order to be time and resource efficient, this is integrated into the code I provide, in my case due to the scale of this example using my 12 cpu's were enough. Each CPU scraps one url, so I'm able to be running 12 url scraps at the same time.\n",
        "\n",
        "This could be increased using a cloud provider, where you can get virtually unlimited computing capacity and cpu's, also the code could be tweaked to be even more efficient and have multithreads inside the multiprocesses, so the scale this bots can reach is quite high, around the tens of millions of urls.\n",
        "\n",
        "Our data will be stored in a csv, but this step could easily be an insert of the data into a relational database. Because don't forget it, another advantatge of this data collection is that our data is structured, we are getting the fields we want and that can be easily modeled as relations and the fields have fixed data-types.\n",
        "\n",
        "Basically were are getting valuable structured data from unstructured public data.\n",
        "\n",
        "The collection of the data in this example has not commercial purposes, it's solely done as an example and only a small section of the domain was scraped while being respectful with the server and not causing any problems to the host. Also the tutorial does not have any cost."
      ],
      "metadata": {
        "id": "ECRC1ybcEsLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code structure and explanation**"
      ],
      "metadata": {
        "id": "u49e_r8pJOAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all  necessary libraries and modules.\n",
        "\n",
        "Mainly:\n",
        "\n",
        "- Selenium in order to render the JavaScript and be able to scroll down in the target webpages. We also use some utilities for waiting while interacting with the webpage, so we seem more human and meanwhile we let time to the code so it can get rendered. Also we import settings related to user agents and conexion related, those settings are used to make our scraper look as a human.\n",
        "\n",
        "- Beautifulsoup will be our html code parser. So we can get the data we are interested in from the whole html of the webpage targeted.\n",
        "\n",
        "- Random, re, requests, time, csv, datetime and os as general utilities.\n",
        "\n",
        "- Webdriver managers, we get the specifical drivers we want our selenium engine to scrap on. Mainly Chrome.\n"
      ],
      "metadata": {
        "id": "6W3WbjfGJa14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcZNh0x9JR4J"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a list of proxies. If you do this using your IP, the server will block you after 3/4 automatic requests in order to avoid so I use a proxy provider to get a list of residential proxies so I make each connection from a different IP, hence a different location hence the server doesn't block my scraper because it views it as a human user.\n",
        "\n",
        "My proxy provider is a highly recognized one: https://smartproxy.com/\n",
        "\n",
        "In this case I have just to specify the proxies because MY IP is whitelisted in my proxy provider, so when the proxy provider server detects its my IP the one that is trying to connect it allows the connection, if you choose another proxy provider this might work in another way.\n",
        "\n",
        "In my case those are residential proxies from Spain, high quality IP's that won't make you look like a robot."
      ],
      "metadata": {
        "id": "zSAmjH-NJhd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "proxy_list = [('es.smartproxy.com',10001,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10002,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10003,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10004,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10005,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10006,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10007,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10008,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10009,\"CHROME\"),\n",
        "              ('es.smartproxy.com',10010,\"CHROME\")]"
      ],
      "metadata": {
        "id": "K8u34YwVJljT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to do functionally decompose the whole process, meaning that for each major step of the process we are going to define a function that does specifically that step. So in case of errors, it's easier to troubleshoot those because we specifically know in which part of the process it was produced and we can go to the function that handles that and fix the code.\n",
        "\n",
        "The smartproxy function sets the options of our webdriver that will be used for the scraping.\n",
        "\n",
        "We basically enable the rendering of JavaScript, so the JavaScript dependent elements such as images or scrolling down are displayed and so we are able to get data from them.\n",
        "\n",
        "We also set the user agent as a human one. If you do not specify a user-agent, the scraper will use the default Python one, showing to the server that we are a bot instead of a human, the server will then reject our connexion, by faking a human user-agent we are able to seem human to the server.\n",
        "\n",
        "Finally, we set the proxy options(HOSTNAME, PORT, DRIVER) through which the scraper will be able to connect to internet."
      ],
      "metadata": {
        "id": "hODG46IRLkX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smartproxy(HOSTNAME, PORT, DRIVER):\n",
        "    \"\"\"\n",
        "    Sets up options for a Chrome driver such as user-agent, enabled javascript or proxy details.\n",
        "\n",
        "    Parameters:\n",
        "      - HOSTNAME: Name of our proxy direction\n",
        "      - PORT: Number of port through which we make the connection to the proxy.\n",
        "      - DRIVER: Driver we are going to use(Chrome, Firefox, Safari)\n",
        "\n",
        "    Returns:\n",
        "      The options object already configured ready to be applied to a driver.\n",
        "    \"\"\"\n",
        "    # Instantiate an ChromeOptions object (We are going to use Chrome driver, others could be used)\n",
        "    options = ChromeOptions()\n",
        "    # Activate javascript rendering\n",
        "    options.add_argument(\"--enable-javascript\")\n",
        "    # Create a fake human user-agent\n",
        "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
        "    # Specifies the mode of the Selenium engine, if headless is selected as I do in the following code line, then while scraping our browser doesn't pop up, in case of experimentation or solving errors, comment the line so\n",
        "    # you are able to see what the scraper is doing visually.\n",
        "    options.add_argument(\"--headless\")\n",
        "    # Disable blink features\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    # Configure the driver to use our proxy details to connect to the internet\n",
        "    proxy_str = '{hostname}:{port}'.format(hostname=HOSTNAME, port=PORT)\n",
        "    options.add_argument('--proxy-server={}'.format(proxy_str))\n",
        "    # We are not interested in images, so for efficiency reasons I disable the loading of images\n",
        "    options.add_argument(\"--disable-image-loading\")\n",
        "\n",
        "    return options"
      ],
      "metadata": {
        "id": "sp0BI1fLLkfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next function called webdriver_instance creates the driver we are going to use for the selenium engine and applies to that driver the options we specify in the smartproxy function, that is applied inside the webdriver_instance function.\n",
        "\n",
        "So firstly, we pick a random proxy from our proxy_list previously specified.\n",
        "We get its hostname, port and driver.\n",
        "\n",
        "Then, we instantiate a Chrome driver and we set its options to be the ones provided by the previous smart_proxy function provided above.\n",
        "\n",
        "We are already able to create a driver with custom options."
      ],
      "metadata": {
        "id": "i22AOfMGNXgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def webdriver_instance():\n",
        "    \"\"\"\n",
        "    Creates an instance and configures the options of a driver ready for Selenium to be used.\n",
        "    \"\"\"\n",
        "    # Get a random value from the list of proxies.\n",
        "    hostname, port, driver = random.choice(proxy_list)\n",
        "    # Instantiate a Chrome driver and configure the options using the previously defined smartproxy function.\n",
        "    browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()),\n",
        "                                   options=smartproxy(hostname, port, driver))\n",
        "\n",
        "    return browser"
      ],
      "metadata": {
        "id": "Mavq2XteNXon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are going to define a function that enters our starting point url, searches for the displayable, clicks on it, scrolls down to show all provinces in the displayable and gets all the url's that redirect to each of the provinces.\n",
        "\n",
        "So the function will return a list with the urls that we need to scrap in order to get all the data from the domain.\n",
        "\n",
        "We need to do this for all commercial domains we want to scrap, there might be little changes between commercial domains, for example:\n",
        "\n",
        "  - Maybe the domain you are interested in scraping has a page dedicated to specifically be the catalog, then just access it, you won't need to display anything and get the urls from the href that are contained in the html code directly.\n",
        "\n",
        "  - In case there is also a displayable in the domain you are interested in, then just change the html container where the displayable is located to the actual one for your website and then also change the anchor tags where the url of the categories are located.\n",
        "  As an example you would have to change:\n",
        "  This line:\n",
        "  dropdown = driver.find_element(By.CLASS_NAME, \"sui-MoleculeSelectPopover-select\")\n",
        "  Change this class name where my catalog is contained: \"sui-MoleculeSelectPopover-select\" to your actual one.\n",
        "\n",
        "  And then change this line:\n",
        "  link_items = soup.find_all('a', class_=\"sui-LinkBasic\")\n",
        "  That gets the urls of the categories, change class_=\"sui-LinkBasic\" this parameter for the anchor container where your urls are located."
      ],
      "metadata": {
        "id": "gTEq243gKTFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categories():\n",
        "    \"\"\"\n",
        "    Enters a webpage and gets the urls of all categories, this one clicks a displayable and retrieves the code after\n",
        "    having clicked, as explained maybe your website has not a displayable and lets you access the urls of categories directly,\n",
        "    then you won't need that part.\n",
        "\n",
        "    Returns:\n",
        "      List of urls with all the categories we want to scrap.\n",
        "    \"\"\"\n",
        "    # Instantiate and assign to a variable an already options configured driver using the previously defined function.\n",
        "    driver = webdriver_instance()\n",
        "    # Assign the catalog url to be scraped to a variable\n",
        "    url = \"https://www.fotocasa.es/es/comprar/viviendas/espana/todas-las-zonas/l\"\n",
        "    # Do a get petition to the url using the driver\n",
        "    driver.get(url)\n",
        "    # Sleep, remember that the bot must not perform actions too quickly we want to be seen as humans, so we wait 5 seconds.\n",
        "    time.sleep(5)\n",
        "    # Search for the displayable inside the webpage, adapt the class name to the website you want to scrap.\n",
        "    # In case there is no displayable and the catalog its a page itself, just pass this step and just retrieve the whole code of the page and search for the tags\n",
        "    # that contain the urls that redirect to each category.\n",
        "    dropdown = driver.find_element(By.CLASS_NAME, \"sui-MoleculeSelectPopover-select\")\n",
        "    # Click on the displayable\n",
        "    dropdown.click()\n",
        "    # Get all the html code of the webpage, including the displayable.\n",
        "    displayable_html = driver.page_source\n",
        "    # Parse the html\n",
        "    soup = BeautifulSoup(displayable_html, 'html.parser')\n",
        "    # Search for the specific tags that contain the urls for each category.\n",
        "    link_items = soup.find_all('a', class_=\"sui-LinkBasic\")\n",
        "    # Get the href(attribute that contains an url that redirects) for each tag.\n",
        "    hrefs = [link.get('href') for link in link_items]\n",
        "    for href in hrefs:\n",
        "        print(href)\n",
        "    # Sleep\n",
        "    time.sleep(5)\n",
        "    driver.quit()\n",
        "    # Get the urls\n",
        "    return hrefs"
      ],
      "metadata": {
        "id": "OQPsoWrhKLav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function gets all the urls of the catalog, it's very possible that you'll want to filter based on certain characteristics, so i.e. you may not want all provinces or you may not want certain categories.\n",
        "\n",
        "This function right here filters the list we have retrieved of urls, you can filter using whatever criteria you need, in my case I filter for certain provinces that I'm interested in.\n",
        "\n",
        "Just take a look at the list of urls retrieved by the function previously and exclude the words you are not interested in or just take the ones you are interested in.\n",
        "\n",
        "To adapt this function all you need is a little bit of experimentation meaning, just retrieve your list of urls and see what you want and what you don't want and then adapt the function and check the result. Once you did it, is completely automated for that specific website domain."
      ],
      "metadata": {
        "id": "Kfa_DQFtK-3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_urls(hrefs, provinces):\n",
        "    \"\"\"\n",
        "    Given a list of url and list of words, filters the list so that the urls remaining contain the words we are interested in.\n",
        "    Also filters for several words we do not want the urls to have.\n",
        "\n",
        "    Takes:\n",
        "      - hrefs: list of urls.\n",
        "      - provinces: list of words, we want our urls to include.\n",
        "\n",
        "    Returns:\n",
        "      Desired list of urls.\n",
        "    \"\"\"\n",
        "    search_words = provinces\n",
        "    # Define your pattern to search based in the words you want to search for.\n",
        "    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in search_words) + r')\\b'\n",
        "    # Filter the original list\n",
        "    filtered_strings = [s for s in hrefs if re.search(pattern, s, re.IGNORECASE)]\n",
        "    # Define an empty list to retrieve the new list of filtered urls.\n",
        "    result = []\n",
        "    # Check for words you dont want your urls to have just add and not re.search(r\"obra-nueva\", url) conditions as follows\n",
        "    for url in filtered_strings:\n",
        "        if not re.search(r\"particulares\", url) and not re.search(r\"obra-nueva\", url) and re.match(r\"^/es/comprar/viviendas/\", url):\n",
        "            result.append(url)\n",
        "    return result"
      ],
      "metadata": {
        "id": "gGqG2IO_GZh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our function to store the data.\n",
        "\n",
        "It requires a list of tuples with our data, the province/category the data belongs to  and a counter that basically will tell us which page of the catalog of the specific category the data belongs to.\n",
        "\n",
        "It outputs a csv or appends to an existing csv the data we are interested in.\n",
        "\n",
        "For bigger projects, it would be pretty easy to substitute this function for one that does inserts into a database instead of writing a csv."
      ],
      "metadata": {
        "id": "b2ZBNPQ-N0Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_csv(data, provincia, counter):\n",
        "    '''\n",
        "      This functions writes a list of tuples in a csv.\n",
        "\n",
        "      Inputs:\n",
        "              - data: list of tuples.\n",
        "              - provincia: name of the province to which the data belongs.\n",
        "              - counter: page of the catalog in integer format.\n",
        "      Output:\n",
        "              Fully written the data list of tuples into a csv. Prints output of the execution.\n",
        "    '''\n",
        "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "    # Set to a variable the path where we want to write our csv\n",
        "    csv_file = f\"C:\\\\Users\\\\Usuario\\\\PycharmProjects\\\\PRA1\\\\fotocasa_{current_date}.csv\" # Substitute for yours.\n",
        "\n",
        "    # Check if file exists\n",
        "    file_exists = os.path.isfile(csv_file)\n",
        "\n",
        "    # Write or append to the file if it already exists.\n",
        "    with open(csv_file, mode='a' if file_exists else 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        if not file_exists:\n",
        "            # Create the header\n",
        "            header = [\"Title\", \"Price\", \"Reduction\", \"Rooms\", \"Bathrooms\", \"Meters\", \"Province\"]\n",
        "            writer.writerow(header)\n",
        "\n",
        "        # write data\n",
        "        writer.writerows(data)\n",
        "    # Print message for traceability\n",
        "    return print(f'Csv data written successfully {provincia} {counter}')"
      ],
      "metadata": {
        "id": "T3I0CRiPOMrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we are going to use all the functions defined above in a main process that will do all the subprocesses."
      ],
      "metadata": {
        "id": "kpnwu5NqV6eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(max_pages):\n",
        "  # Get all the urls for each category\n",
        "  provinces = ['tarragona','lleida','barcelona', 'girona']\n",
        "  hrefs = get_categories()\n",
        "  # Filter those urls for the ones we are interested in\n",
        "  urls = filter_urls(hrefs, provinces)\n",
        "  # Define the base url of the domain, the url list we are retrieved will probably be partial urls, meaning you will need to add the base domain url to acces an actual url.\n",
        "  # If this is different for your website and it gives you full urls, you don't need to do this, otherwise you'll need to figure out what you have to add to those partial urls\n",
        "  # to be able to access the actual urls.\n",
        "  base_url = \"https://www.fotocasa.es\"\n",
        "  # For every partial url, we get the province/category we are going to scrap, you'll need to figure out where in your partial urls this province/category is contained\n",
        "  # For each of our partial urls we are going to do the following process.\n",
        "  for i in urls:\n",
        "      parts = i.split('/')\n",
        "      for part in parts:\n",
        "          if '-' in part:\n",
        "              element = part.split('-')[0]\n",
        "              break\n",
        "      # Get the specific province of that url\n",
        "      provincia = element\n",
        "      # Set up a counter, this will be used to monitor how many pages in the catalog we want to deep in, each page of a given category will increase the counter in 1 until\n",
        "      # we reach our max_pages parameter where the process will stop because we are not interested in deepening more in the catalog, maybe you want the full catalog, then just setup\n",
        "      # a big number as the parameter.\n",
        "      counter = 0\n",
        "      # While the counter is <= than the max_pages parameter we'll keep scraping pages of the catalog.\n",
        "      while counter <= max_pages:\n",
        "        # If its the starting url for the category, we are simply getting the base_url + the partial url retrieved from the displayable or catalog starting page.\n",
        "          if counter == 0:\n",
        "              url = base_url + i\n",
        "        # If its not the starting url, we are getting the base_url + the partial url obtained from getting the next link of the catalog\n",
        "          else:\n",
        "              url = base_url + next_link\n",
        "          # Time the process\n",
        "          start_time = time.time()\n",
        "          # Create a webdriver\n",
        "          driver = webdriver_example()\n",
        "          # Access the page\n",
        "          driver.get(url)\n",
        "          # Sleep, adapt your sleeps so the server doesn't block you but it also doesn't take an eternity to scrap all pages\n",
        "          time.sleep(5)\n",
        "          # Once we are on the page we are going to scroll down to show all its content\n",
        "          scroll_height = 0\n",
        "          while True:\n",
        "              driver.execute_script(\"window.scrollTo(0, arguments[0]);\", scroll_height)\n",
        "              time.sleep(2)\n",
        "              scroll_height += 1000\n",
        "              if scroll_height >= driver.execute_script(\"return document.body.scrollHeight\"):\n",
        "                  break\n",
        "         # Once we have scrolled the whole page, we are retrieving the page html code\n",
        "          page_source = driver.page_source\n",
        "          # Sleep, again adapt to your necessities\n",
        "          time.sleep(7)\n",
        "          # Close the driver\n",
        "          driver.quit()\n",
        "          # Parse the html code\n",
        "          soup = BeautifulSoup(page_source, 'html.parser')\n",
        "          # Get the containers that have each advertisement. That's usually a common structure in comercial websites, offers\n",
        "          # will be contained in containers of the same class. Get all of these containers, each one is an advertisement.\n",
        "          info_divs = soup.find_all('div', class_='re-CardPackPremium-info') # Change the name of the class for the specific name of the class of your website\n",
        "          # If there are enough containers just pass and apply the normal process, because that means we have targeted the right class to get the containers.\n",
        "          if len(info_divs) > 10:\n",
        "              pass\n",
        "          # Else means that was not the right class because we have not retrieved enough containers, try with a nother class where they might be.\n",
        "          else:\n",
        "              info_divs = soup.find_all('div', class_='re-CardPackAdvance-info') # Substitute for your specific class.\n",
        "          # Create a list to store data\n",
        "          flats = []\n",
        "          # For each container get the data we are interested in.\n",
        "          for info_div in info_divs:\n",
        "              # Get the title of the advertisement\n",
        "              # In order for an advertisement to exist it must have a title\n",
        "              title = info_div.find('span', class_=\"re-CardTitle re-CardTitle--big\").text # Substitute for your specific class inside the container that contains the title.\n",
        "              # Get the price of the advertisement, targeting the class that contains the data, if it does not exist, mark the price as null\n",
        "              precio_span = info_div.find('span', class_=\"re-CardPrice\")\n",
        "              price = precio_span.text if precio_span else \"-\"\n",
        "              # Get the reduction of the price, targetting the class that contains the data, if it does not exists, mark the reduction as null\n",
        "              reduccio_span = info_div.find('span', class_=\"re-CardPriceReduction\")\n",
        "              reduccio = reduccio_span.text if reduccio_span else \"-\"\n",
        "              # Get the rooms of the property, targetting the class that contains the data, if it does not exists, mark the rooms as null\n",
        "              rooms_span = info_div.find('span', class_=\"re-CardFeaturesWithIcons-feature-icon re-CardFeaturesWithIcons-feature-icon--rooms\")\n",
        "              rooms = rooms_span.text if rooms_span else \"-\"\n",
        "              # Get the bathrooms of the property, targetting the class that contains the data, if it does not exists, mark the bathrooms as null\n",
        "              bathrooms_span = info_div.find('span', class_=\"re-CardFeaturesWithIcons-feature-icon re-CardFeaturesWithIcons-feature-icon--bathrooms\")\n",
        "              bathrooms = bathrooms_span.text if bathrooms_span else \"-\"\n",
        "              # Get the meters of the property, targetting the class that contains the data, if it does not exists, mark the meters as null\n",
        "              meters_span = info_div.find('span', class_=\"re-CardFeaturesWithIcons-feature-icon re-CardFeaturesWithIcons-feature-icon--surface\")\n",
        "              meters = meters_span.text if meters_span else \"-\"\n",
        "              # Save this data in a list as a tuple, at the end of the for loop this list will have the tuples with the all the data of the specific page of the catalog of the category\n",
        "              flats.append((title, price, reduccio, rooms, bathrooms, meters, provincia))\n",
        "          # Store the data in a csv using the previously defined function\n",
        "          get_csv(flats,provincia, counter)\n",
        "          # Get the partial url of the next page in the catalog\n",
        "          li_tags = soup.find_all('li', class_='sui-MoleculePagination-item') # Target the container that has the url redirecting to next page. usually located at the Next button of the catalog.\n",
        "          # We assume it does not exists\n",
        "          href = ''\n",
        "          # Get the anchor tag containing the next url, extract the href, it might be contained in several containers, we check each one and get the one that is not empty\n",
        "          for li_tag in li_tags:\n",
        "              a_tag = li_tag.find('a', class_='sui-AtomButton sui-AtomButton--primary sui-AtomButton--outline sui-AtomButton--center sui-AtomButton--small sui-AtomButton--link sui-AtomButton--empty sui-AtomButton--rounded') # Change for your targeted class\n",
        "              if a_tag:\n",
        "                  href = a_tag.get('href')\n",
        "              else:\n",
        "                  pass\n",
        "          # Save the partial url for next iteration\n",
        "          next_link = href\n",
        "          # Get finishing time\n",
        "          end_time = time.time()\n",
        "          # Get total time of 1 iteration\n",
        "          execution_time = end_time - start_time\n",
        "          # Print which page we have scraped\n",
        "          print(f'Page {counter + 1} of {provincia} scraped')\n",
        "          # Print how much time it took\n",
        "          print(f\"Execution time: {execution_time:.4f} seconds\")\n",
        "          # Print how many advertisements we have collected data from\n",
        "          print(len(flats))\n",
        "          # Raise the counter, so we know we have passed a page in the catalog\n",
        "          counter += 1\n",
        "          # Sleep, adapt to your necessities\n",
        "          time.sleep(20)\n",
        "          # If we reach the max_pages parameter in our catalog stop the process and go to the next url obtained from the catalog of categories.\n",
        "          if counter > max_pages:\n",
        "              break\n",
        "  # Print finish of the process\n",
        "  return print('Succesful')"
      ],
      "metadata": {
        "id": "yDPhRnJYWFII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a nutshell, this code structure will work for all commercial websites with a catalog.\n",
        "\n",
        "You must customize certain code aspects to meet the requirements of your specific website.\n",
        "\n",
        "Those customization can be resumed as:\n",
        "\n",
        "  - The domain has an specific url for the catalog?\n",
        "      - Yes, then modify the get_categories function to skip the part where we display the displayable.\n",
        "  - You must also change all the html classes, including anchors and hrefs, to adapt them to your target domain structure. Maybe there is certain functionalities that might be different such as getting the next link, maybe your next link has a certain container where is located, then modify the code to adapt it to your specific necesites.\n",
        "  - Adapt the data extraction to the elements you want to extract, if there are more add, if they are different modify, etc.\n",
        "\n",
        "Other than that, the structure can be applied to mainly all commercial websites in order to scrap its data."
      ],
      "metadata": {
        "id": "jEQundvlKusi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Getting bigger in scale**"
      ],
      "metadata": {
        "id": "jcsRt-fPMRCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are planning to do this in a big scale, you will have to adapt the function for each website.\n",
        "\n",
        "Maybe you want to monitor 10, 15 websites. Then adapt your function for each one of them, do testing in order to check you are extracting the data alright.\n",
        "\n",
        "Once you have a function for each web domain you want to scrap, then you'll probably need to run those functions in multiprocessing due to time and efficiency.\n",
        "\n",
        "As I said my computer has 12 cpu's, I always leave atleast 1 or 2 cpu's for other purposes, so count that I can have 10 cpu's dedicated to this process.\n",
        "Then I can carry simultaneuosly 10 different webscraping processes.\n",
        "\n",
        "In a code example it would be."
      ],
      "metadata": {
        "id": "dIB79BemMdMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a list of functions\n",
        "    functions = [func1, func2, func3, func4, func5, func6, func7, func8, func9, func10]\n",
        "\n",
        "    # Create a pool of worker processes\n",
        "    with multiprocessing.Pool() as pool:\n",
        "        # Use the pool to map the functions to the worker processes\n",
        "        # This will run the functions in parallel\n",
        "        pool.map(lambda f: f(), functions)"
      ],
      "metadata": {
        "id": "OL0uZ147NCkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need to pass any arguments to the global main function we'll have for each web domain we want to scrap, so just run all the 10 processes at the same time with a multiprocessing pool.\n",
        "\n",
        "If you wanted to get tricky and become much more efficient, you can multithread at the same time,  some of the subprocesses that are happening inside each main function. This would make the code run much faster.\n",
        "\n",
        "Also you'll probably need to handle errors with try blocks. But as I said before it all comes down to testing each function and see if it works and fix in case it doesn't once you have tested each function the bot will give you huge data collection capacity and you'll have to change literally nothing for months while being able to extract valuable data.\n",
        "\n",
        "The only costs associated with this process is computation capacity and the proxy provider, by having a good deal with your proxy provider, once you have already defined your functions for each target website, you'll be able to retrieve valuable data 24/7 all year long while applying little to no effort and at a very low economic cost. Probably this data will provide you with business insights that could not be achieved otherwise because this is real time information."
      ],
      "metadata": {
        "id": "DxaEj9J1ORaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Results**"
      ],
      "metadata": {
        "id": "HagSiowEPZxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I executed this example with a depth of 4 pages into the catalog for category/province. I filtered and only used 4 provinces that are the ones that compose the specific region where I live.\n",
        "\n",
        "The result was this csv:\n",
        "\n",
        "https://github.com/zetag33/FotoCasa/blob/main/dataset/fotocasa_2023_11_06.csv\n",
        "\n",
        "Structured and valuable market data at a cheap cost.\n",
        "\n"
      ],
      "metadata": {
        "id": "hiIetIvPPfP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Particular use-case 2**"
      ],
      "metadata": {
        "id": "VcGEr7wEuIXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Explanation**"
      ],
      "metadata": {
        "id": "hOnPQzqbuRVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's imagine we do not need want all data in the whole catalog of the whole page, we do not even want all the info about a category, we are just interested in one single product.\n",
        "\n",
        "This product can be searched for in the searchbox of the webpage.\n",
        "And then we are automatically redirected to an url that contains all the advertisements for that specific product.\n",
        "\n",
        "This is also an interesting use-case that will repeat itself for many webpages, so just let's develop the infrastructure for this use-case so we have to do minimum changes between domains.\n",
        "\n",
        "We are going to test it in WallaPop a Spanish second-hand market of a wide range of products.\n",
        "\n",
        "https://es.wallapop.com/\n",
        "\n",
        "Our webpage has a peculiarity, once you search for a specific product, the webpage you are redirected to, has no catalog, it shows more products by scrolling and not by clicking to go to the next page in a catalog, the scrolling also is infinite you can't reach the end, it will let you scroll forever just by repeating the products shown.\n",
        "\n",
        "We are going to solve the scrolling issue by setting a timer, once we have scroll for a desired time, the bot will stop, it's our responsibility to fix how much time is the optimum, this will be known by testing.\n",
        "\n",
        "In case our target webpage doesn't behave like that which is quiet likely, we should replace the scrolling code with the iteration of the catalog code shown in the real state bot. I'll show an example of how we would do that.\n",
        "\n"
      ],
      "metadata": {
        "id": "28-LYcoeudyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code infrastructure**"
      ],
      "metadata": {
        "id": "MmRW-bDQvUk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are again going to use Python and Selenium which fit perfect for this use-case.\n",
        "\n",
        "We are basically going to:\n",
        "\n",
        "    - Enter a webpage.\n",
        "    - Write in the searchbox.\n",
        "    - Click Search into the searchbox.\n",
        "    - Enter the redirect url.\n",
        "    - Scroll down until the end of the page or until we are not interested anymore in the data.\n",
        "    - Retrieve the page code.\n",
        "    - Extract our desired data.\n",
        "    - Store the data."
      ],
      "metadata": {
        "id": "pPRs1Vykvdru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's then start by importing our dependencies, they will be mainly the same as in the bot based in the catalog."
      ],
      "metadata": {
        "id": "hl0kL094wFtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "import selenium\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import os\n",
        "import concurrent.futures\n",
        "import threading"
      ],
      "metadata": {
        "id": "Nsg_Or1ov5jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are then going to define to functions we have already seen in the catalog bot.\n",
        "\n",
        "There is a difference in this use-case compared to the past one.\n",
        "\n",
        "We are not going to specifically need proxies for this one. We are only making two connections. One for the search and another for the redirect.\n",
        "\n",
        "And most important, the url that redirects to the search results doesn't have a catalog, so we don't have to enter a different website each time. That's the main reason we are not using proxies here, the server won't block us for 2 connexions for each time we search for a specific product.\n",
        "\n",
        "If needed we could just take the code of the first use case bot and add the proxy part."
      ],
      "metadata": {
        "id": "QKDqEH8vwTho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the same 2 functions we had for the catalog bot.\n",
        "\n",
        "One to set-up the options of our driver and another one to set up the driver already configured.\n",
        "\n",
        "The only difference is here we are not using proxy details, if needed use the functions defined before."
      ],
      "metadata": {
        "id": "hwlxZ7BIxtnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def options():\n",
        "    \"\"\"\n",
        "    Sets up an options object for a Chrome driver such as user-agent, enabled javascript or proxy details.\n",
        "    \"\"\"\n",
        "    # Instantiate an options object\n",
        "    options = ChromeOptions()\n",
        "    # Activate javascript rendering\n",
        "    options.add_argument(\"--enable-javascript\")\n",
        "    # Use a \"human\" user-agent\n",
        "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
        "    # Comment this line for production environments, this makes the browser pop up in your screen so you can inspect the process\n",
        "    options.add_argument(\"--headless\")\n",
        "    # Disable blink features and various\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    options.add_argument(\"--disable-image-loading\")\n",
        "    options.add_argument(\"--disable-extensions\")\n",
        "    options.add_argument(\"--disable-plugins\")\n",
        "    options.add_argument(\"--disable-popup-blocking\")\n",
        "    return options"
      ],
      "metadata": {
        "id": "A-ohc0-UwSlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def webdriver_example():\n",
        "  '''\n",
        "  Creates an already options configured Chrome driver and returns the object\n",
        "  '''\n",
        "  # Instantiate the driver using the options\n",
        "  browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()),\n",
        "                              options=options())\n",
        "\n",
        "  return browser"
      ],
      "metadata": {
        "id": "a9NOuuQUyqPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to specifically create a function for the scrolling part."
      ],
      "metadata": {
        "id": "93aaWPfmzFZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scroll_down_slowly(driver):\n",
        "  '''\n",
        "  Scrolls down a webpage once the get request of a driver is done\n",
        "  '''\n",
        "  # Scroll\n",
        "  driver.execute_script(\"window.scrollBy(0, 100);\")\n",
        "  # Wait between scrolls to seem more human\n",
        "  time.sleep(0.5)  # Adjust the sleep time based on your preference"
      ],
      "metadata": {
        "id": "CMJWOC9BzKkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now given that we don't need to access a catalog. We'll get all the webpage we are interested in a single function. I'll also provide a version of this function adapted to a case that when you are redirected to the search of a specific product there is a catalog in those results. As said before in this one there is not, you just scroll until you want to stop and products are displayed."
      ],
      "metadata": {
        "id": "PMttbdy40BaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_code(palabra: str):\n",
        "  '''\n",
        "  Given a word(the product we are interested in), acceses the root url, writes in the\n",
        "  searchbox, clicks for search, displays items, scrolls down until we are interested in and extracts webpage code\n",
        "  with the data we are interested in\n",
        "  '''\n",
        "  # Create a webdriver\n",
        "  driver = webdriver_example()\n",
        "  # Do the get petition to the url root domain\n",
        "  driver.get(\"https://es.wallapop.com/\")\n",
        "  # Gets the searchbox\n",
        "  search_box = driver.find_element(By.ID, \"searchbox-form-input\")\n",
        "  search_query = palabra\n",
        "  # Writes into the searchbox\n",
        "  search_box.send_keys(search_query)\n",
        "  search_box.send_keys(Keys.RETURN)\n",
        "  time.sleep(5)\n",
        "  target_class = \"btn-load-more\"\n",
        "  # Searches for the button of search\n",
        "  button = driver.find_element(By.ID, target_class)\n",
        "  # Clicks on it\n",
        "  if button:\n",
        "      button.click()\n",
        "  time.sleep(10)\n",
        "  scroll_height = 0\n",
        "  start_time = time.time()\n",
        "  # Scrolls down\n",
        "  while True:\n",
        "      driver.execute_script(\"window.scrollTo(0, arguments[0]);\", scroll_height)\n",
        "      # Adapt sleeps to your necessities\n",
        "      time.sleep(5)\n",
        "      # Adapt how much you scroll each time to your necessities\n",
        "      scroll_height += 500\n",
        "      # When reaches bottom of the webpage stops scrolling(here you'll never reach the bottom since there is no bottom)\n",
        "      if scroll_height >= driver.execute_script(\"return document.body.scrollHeight\"):\n",
        "          break\n",
        "      elapsed_time = time.time() - start_time\n",
        "      # If we have been scrolling for more than 30 seconds, stop, adapt this to your necessities\n",
        "      if elapsed_time >= 30:\n",
        "          break\n",
        "  # Gets webpage code\n",
        "  info = driver.page_source\n",
        "  driver.quit()\n",
        "  return info"
      ],
      "metadata": {
        "id": "DlgWupB50b74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we got all the info, we have to parse it in order to get only the data we are interested in."
      ],
      "metadata": {
        "id": "YWYEXdAA2Rfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_code(info):\n",
        "    '''\n",
        "    Given the retrieved code of the webpage parses the content to get title and price\n",
        "    '''\n",
        "    # Create a list to store the data\n",
        "    data = []\n",
        "    # Parse the content\n",
        "    soup = BeautifulSoup(info, 'html.parser')\n",
        "    # Get all the containers that contain the data for each product\n",
        "    containers = soup.find_all('div', class_='ItemCard__data')\n",
        "    # For each one\n",
        "    for container in containers:\n",
        "        # Extract price information\n",
        "        price_span = container.find('span', class_='ItemCard__price')\n",
        "        price = price_span.text.strip() if price_span else \"N/A\"\n",
        "\n",
        "        # Extract title information\n",
        "        title_p = container.find('p', class_='ItemCard__title')\n",
        "        title = title_p.text.strip() if title_p else \"N/A\"\n",
        "        data.append((title, price))\n",
        "    return data"
      ],
      "metadata": {
        "id": "4vD5kIni2W6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now simply store this data into a csv with the same function we used in the previous use case"
      ],
      "metadata": {
        "id": "wuy4Y4t42ySp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_csv(data, palabra):\n",
        "    '''\n",
        "    Given the extracted data and our search target writes a csv with the data.\n",
        "    '''\n",
        "    current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "    # Path where you want the csv file to be.\n",
        "    csv_file = f\"C:\\\\Users\\\\ROG STRIX\\\\PycharmProjects\\\\pythonProject\\\\wallapop_{palabra}_{current_date}.csv\"\n",
        "\n",
        "    # Check if this path already exists\n",
        "    file_exists = os.path.isfile(csv_file)\n",
        "\n",
        "    # If if already exists we append to the file else we create it\n",
        "    with open(csv_file, mode='a' if file_exists else 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        if not file_exists:\n",
        "            # Create a header\n",
        "            header = [\"Title\", \"Price\"]\n",
        "            writer.writerow(header)\n",
        "\n",
        "        # Write data\n",
        "        writer.writerows(data)\n",
        "    # Print for traceability\n",
        "    return print(f'Csv data written successfully {palabra}{current_date}')"
      ],
      "metadata": {
        "id": "GwNPkPRf26mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally put it all together in a single process and call for the execution."
      ],
      "metadata": {
        "id": "ts3tppVG3YxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(palabra: str):\n",
        "    info = get_code(palabra)\n",
        "    data = parse_code(info)\n",
        "    get_csv(data, palabra)\n",
        "\n",
        "main(\"secador dyson\")"
      ],
      "metadata": {
        "id": "JdIEsU0p3bGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will open our root url, write \"secador dyson\" that is a specific hair dryer. Redirect to the results url and get the data of all the results we want until we stop scrolling using the time parameter.\n",
        "\n",
        "Like that we have easily got real time data of a product we might be interested in, so we are able to know the average price of the given product in the market, the number of advertisements there are, the location of them etc...\n",
        "Execution is so easy and results are pretty clean.\n",
        "\n",
        "If we were interested we can multiprocess it easily, since it's a single function and search for various products we are interested in, we could even do NLP into the description of the offers or even some kind of Computer Vision using the images of the offers, it can get as complex as you want."
      ],
      "metadata": {
        "id": "2spx_2nf3mPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Result**"
      ],
      "metadata": {
        "id": "iuOSoiih4ZLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extracted csv of the process is this one.\n",
        "We have extracted 81 offers of this product, the timer is there because products repeat itself once you have scrolled for a while so we are only interested in those because they are the uniques.\n",
        "https://github.com/zetag33/WebScraping-Tutorial-Use-Case/blob/main/dataset/wallapop_secador%20dyson_2023_11_29.csv"
      ],
      "metadata": {
        "id": "YUCelG6Q5XQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Catalog variation**"
      ],
      "metadata": {
        "id": "Uyivhe095xiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's imagine that instead of an ethernal scrolldown you got a catalog as search results. Then we'll have to scroll down until the bottom, capture the next link to the next page of the catalog and do the same. Let's also provide a function to do that."
      ],
      "metadata": {
        "id": "SKTGO2Ue1aBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_code_catalog_variation(palabra: str, next_link=''):\n",
        "  '''\n",
        "  Given a word(the product we are interested in), acceses the root url, writes in the\n",
        "  searchbox, clicks for search, displays items, scrolls down until we are interested in and extracts webpage code\n",
        "  with the data we are interested in. If we already got a next_link, simply accesses the next_link, scroll and gets its data.\n",
        "  '''\n",
        "  if next_link != '':\n",
        "    driver = webdriver_example()\n",
        "    driver.get(next_link)\n",
        "    # Here differences start.\n",
        "    scroll_height = 0\n",
        "    start_time = time.time()\n",
        "    # Scrolls down till the end of the page\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, arguments[0]);\", scroll_height)\n",
        "        # Adapt sleeps to your necessities\n",
        "        time.sleep(5)\n",
        "        # Adapt how much you scroll each time to your necessities\n",
        "        scroll_height += 500\n",
        "        # When reaches bottom of the webpage stops scrolling(here you'll never reach the bottom since there is no bottom)\n",
        "        if scroll_height >= driver.execute_script(\"return document.body.scrollHeight\"):\n",
        "            break\n",
        "    # Gets webpage code\n",
        "    info = driver.page_source\n",
        "    driver.quit()\n",
        "  else:\n",
        "    ## Remains the same\n",
        "    # Create a webdriver\n",
        "    driver = webdriver_example()\n",
        "    # Do the get petition to the url root domain\n",
        "    driver.get(\"https://es.wallapop.com/\")\n",
        "    # Gets the searchbox\n",
        "    search_box = driver.find_element(By.ID, \"searchbox-form-input\")\n",
        "    search_query = palabra\n",
        "    # Writes into the searchbox\n",
        "    search_box.send_keys(search_query)\n",
        "    search_box.send_keys(Keys.RETURN)\n",
        "    time.sleep(5)\n",
        "    target_class = \"btn-load-more\"\n",
        "    # Searches for the button of search\n",
        "    button = driver.find_element(By.ID, target_class)\n",
        "    # Clicks on it\n",
        "    if button:\n",
        "        button.click()\n",
        "    time.sleep(10)\n",
        "    # Here differences start.\n",
        "    scroll_height = 0\n",
        "    start_time = time.time()\n",
        "    # Scrolls down till the end of the page\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, arguments[0]);\", scroll_height)\n",
        "        # Adapt sleeps to your necessities\n",
        "        time.sleep(5)\n",
        "        # Adapt how much you scroll each time to your necessities\n",
        "        scroll_height += 500\n",
        "        # When reaches bottom of the webpage stops scrolling(here you'll never reach the bottom since there is no bottom)\n",
        "        if scroll_height >= driver.execute_script(\"return document.body.scrollHeight\"):\n",
        "            break\n",
        "    # Gets webpage code\n",
        "    info = driver.page_source\n",
        "    driver.quit()\n",
        "  return info"
      ],
      "metadata": {
        "id": "oPgpsm3T1odC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now create a function that will return the next link of the catalog"
      ],
      "metadata": {
        "id": "-Fg2S4P6_UfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_link(info):\n",
        "  '''\n",
        "  Given the webpage source code, returns next link of the catalog\n",
        "  '''\n",
        "  # Parse the code\n",
        "  soup = BeautifulSoup(info, 'html.parser')\n",
        "  # Get all the containers that contain the data for each product\n",
        "  next_link = soup.find('a', class_='next_link').get(href)# Usually its an a tag and it's located inside an href, substitute for the location of yours\n",
        "  return next_link"
      ],
      "metadata": {
        "id": "5fbCMjVe_Xfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's put it all together in a main function"
      ],
      "metadata": {
        "id": "rMst371NAVNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(word, max_pages):\n",
        "  counter = 0\n",
        "  # Start with the first page\n",
        "  info = get_code_catalog_variation(word)\n",
        "  data = parse_code(info)\n",
        "  get_csv(data, word)\n",
        "  # Get next_link\n",
        "  next_link = get_next_link(info)\n",
        "  while counter <= max_pages and next_link != ''#Exists:\n",
        "    # Change this condition according to your necessities, this will make it stop at 5th iteration inside the catalog given next_link exists\n",
        "    info = get_code_catalog_variation(word, next_link)\n",
        "    data = parse_code(info)\n",
        "    get_csv(data, word)\n",
        "    next_link = get_next_link(info)\n",
        "    counter += 1"
      ],
      "metadata": {
        "id": "f2u3SrqkAXm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has also been an example of how easily can you change the structure of the code based in your necessities. You only have to pick up which parts are usefuls and which ones you should add or modify. There is plenty of examples in the notebook for you to have code structure for a huge part of the commercial webscraping possibilities."
      ],
      "metadata": {
        "id": "OBiOadk5DIqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Particular use-case 3**"
      ],
      "metadata": {
        "id": "BAa079o3Dxor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Explanation**"
      ],
      "metadata": {
        "id": "sqw2HiKvFLU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do our last use-case.\n",
        "\n",
        "Now we'll scrap a catalog website again, the difference will be that the catalog is not a displayable, the webpage will have a dedicated url for the catalog where you have to get the links from and you don't need to display nothing, then access all those urls and get the data.\n",
        "\n",
        "This one was quite a difficult one because of the web structure. The codes provided here are general guidelines but as said many times before you'll need to adapt it to the specific structure of your website. Meaning sometimes you'll have to use tricks and understand the situation.\n",
        "\n",
        "We are going to access PAGINAS AMARILLAS a Spanish website that offers the contact info of businesses and particulars that offer services and we are going to get its data segmentated by categories.\n",
        "\n",
        "This is our starting point: https://www.paginasamarillas.es/all_tarragona_reus.html\n",
        "\n",
        "A catalog. We could do it on the catalog that is for all spain, I'm doing in this one because it is my province and was what i was interested in at the moment.\n",
        "\n",
        "If you enter the url provided, you'll see that this example is much more difficult because we'll need to get firstly the url's for each town/village/city and then get the specific url for each service in each town/village/city. This is kind of a stack of catalogs.\n",
        "\n",
        "This will amount to more than 10.000 url's we are going to access.\n",
        "\n",
        "This example will be tricky, because we'll see that many times containers change classes between different pages in the catalog so we'll check if we have data and in case we don't then we'll try another approach in order to get it. I have checked that when you don't have it the first way you will be able to have it the second way. As I explained above, webscraping is all about being creative and finding tricks as well as understanding the situation.\n",
        "\n",
        "This example will also intrinsecaly involve multiprocessing.\n",
        "\n",
        "So let's solve this much more difficult challenge."
      ],
      "metadata": {
        "id": "oGjwORfpD3fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Code Infrastructure**"
      ],
      "metadata": {
        "id": "Ebobr6z9GAeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, as always, import our dependencies, they are the same as in the other 2 use-cases."
      ],
      "metadata": {
        "id": "bW6g2IhOGUIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from webdriver_manager.firefox import GeckoDriverManager\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "import os\n",
        "import builtwith\n",
        "import concurrent.futures\n",
        "import threading"
      ],
      "metadata": {
        "id": "QG9mS4RdFKWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, define our 2 functions to create the driver, one to define options another one to instantiate a driver with those options. We've already also seen these functions for the previous examples"
      ],
      "metadata": {
        "id": "CnfaB-JoGjNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def options():\n",
        "\n",
        "    options = ChromeOptions()\n",
        "    # Activem javascript\n",
        "    options.add_argument(\"--enable-javascript\")\n",
        "    # Associem al driver un useragent hum\n",
        "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
        "    options.add_argument(\"--headless\")\n",
        "    # Desactivem les blink features\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    # Configurem la proxy a las opcions del driver\n",
        "    # Desactivem el carregat d'imatges.\n",
        "    options.add_argument(\"--disable-image-loading\")\n",
        "    options.add_argument(\"--disable-extensions\")\n",
        "    options.add_argument(\"--disable-plugins\")\n",
        "    options.add_argument(\"--disable-popup-blocking\")\n",
        "    return options\n",
        "\n",
        "\n",
        "\n",
        "def webdriver_example():\n",
        "\n",
        "    # El creem amb les opcions de la proxy escollida cridant a la funcio smartproxy anterior.\n",
        "    browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()),\n",
        "                               options=options())\n",
        "\n",
        "    return browser\n"
      ],
      "metadata": {
        "id": "iP8j4KTSG0aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following this, we know that for this case we have a dedicated url for the catalog, so let's access that catalog and extract all the url's we'll need to access, we'll write them in a file."
      ],
      "metadata": {
        "id": "s4cGyLzpG7sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_links_provincia(url):\n",
        "  '''\n",
        "  Given a url for a whole province, extract the url's for each town/city in the province\n",
        "  '''\n",
        "  # Instantiate driver\n",
        "  driver = webdriver_example()\n",
        "  # Access webpage\n",
        "  driver.get(url)\n",
        "  # Get code\n",
        "  page_source = driver.page_source\n",
        "  time.sleep(10)\n",
        "  # Parse it\n",
        "  soup = BeautifulSoup(page_source, 'html.parser')\n",
        "  # Get the containers that contain the hrefs\n",
        "  menuscroll_divs = soup.find_all('div', class_=\"menuscroll\")\n",
        "  hrefs = []\n",
        "  for div in menuscroll_divs:\n",
        "      # Get each anchor tag\n",
        "      anchor_tags = div.find_all('a', href=True)\n",
        "      # Extract the text url\n",
        "      for anchor in anchor_tags:\n",
        "          hrefs.append(anchor['href'])\n",
        "  # Check how many we extracted\n",
        "  print(len(hrefs), hrefs)\n",
        "  return hrefs"
      ],
      "metadata": {
        "id": "WaZW2yj2HJWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the link for each city/town/village we'll need to access each one and get the url that redirects to a given type of service for each city/town/village"
      ],
      "metadata": {
        "id": "wOYwvZb1JU8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_links_localidad(url):\n",
        "  '''\n",
        "  Given a url for a specific town/village/city, extracts the url for each service in the town/village/city\n",
        "  '''\n",
        "  # Instantiate driver\n",
        "  driver = webdriver_example()\n",
        "  # Access webpage\n",
        "  driver.get(url)\n",
        "  # Get the code of the webpage\n",
        "  page_source = driver.page_source\n",
        "  # Parse it\n",
        "  soup = BeautifulSoup(page_source, 'html.parser')\n",
        "  # Get the urls for each service in the town/city/village\n",
        "  ficha_divs = soup.find_all('div', class_=\"ficha\")\n",
        "  hrefs = []\n",
        "  for div in ficha_divs:\n",
        "      p_tags = div.find_all('p')\n",
        "      for p in p_tags:\n",
        "          anchor_tags = p.find_all('a', href=True)\n",
        "          for anchor in anchor_tags:\n",
        "              href = anchor['href']\n",
        "              if 'juzgados' not in href and 'caja-rural' not in href: # We don't want those for specific reasons of the underlying study\n",
        "                  hrefs.append(href)\n",
        "  return hrefs"
      ],
      "metadata": {
        "id": "Kdh-dopOJGAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call it all together and then write a txt with all the url's we'll need to access"
      ],
      "metadata": {
        "id": "IAXbt6CWKATP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the link for each village/city/town\n",
        "localidades = get_links_provincia('https://www.paginasamarillas.es/all_tarragona_reus.html')\n",
        "total_links = []\n",
        "# For each town\n",
        "for i in localidades:\n",
        "    # Get the link of each service\n",
        "    sectores = get_links_localidad(i)\n",
        "    # Store those urls\n",
        "    total_links.append(sectores)\n",
        "# Put it all together in a single list\n",
        "total_links = [item for sublist in total_links for item in sublist]\n",
        "# Write the content of this list in a txt, so we have a document with all the url's we'll have to access.\n",
        "with open('urls.txt', 'w') as file:\n",
        "    for link in total_links:\n",
        "        file.write(link + '\\n')"
      ],
      "metadata": {
        "id": "64VeRj0oKFMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll need to access each one of those urls, extract the data and store it.\n",
        "Those url's can be found here in the txt I generated using this chunk of code:\n",
        "https://github.com/zetag33/paginas_amarillas/blob/main/urls.txt"
      ],
      "metadata": {
        "id": "Yn75RIWtKdkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define some utility functions for this specific case. You'll need to get yours adapting it to your necessities"
      ],
      "metadata": {
        "id": "lPUtYXfJOMXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_last_number(url):\n",
        "  '''\n",
        "  In case there is a certain feature in the url we do not want, remove it.\n",
        "  '''\n",
        "  # Search for the pattern\n",
        "  pattern = r'/(\\d+)$'\n",
        "  match = re.search(pattern, url)\n",
        "  # If found, remove it\n",
        "  if match:\n",
        "      last_number = match.group(1)\n",
        "      url_without_last_number = url[:-len(last_number)]\n",
        "      return url_without_last_number\n",
        "  # Else return the original url\n",
        "  else:\n",
        "      return url"
      ],
      "metadata": {
        "id": "tH87RL9AKhL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the province. Given the url, using patterns extract the province name, we know by watching url structure, that the province will always be located in a specific spot after a key word and followed by another key word, so use this pattern to extract the province name of the url"
      ],
      "metadata": {
        "id": "chyMdUklP11-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_province(url):\n",
        "  ''' Get the province name from the url\n",
        "  '''\n",
        "    # Remove unwanted feature\n",
        "    url = remove_last_number(url)\n",
        "    # Specify a pattern\n",
        "    match = re.search(r'/([^/]+)/([^/]+)/$', url)\n",
        "    # If match, extract the province\n",
        "    if match:\n",
        "        province = match.group(1)\n",
        "    # Else set the province as null\n",
        "    else:\n",
        "        province = None\n",
        "    return province"
      ],
      "metadata": {
        "id": "IuZwNe3kQCcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we do the same for the city"
      ],
      "metadata": {
        "id": "AbXD9qJwQT9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_city(url):\n",
        "  '''\n",
        "  Get the city name from url\n",
        "  '''\n",
        "  # Remove unwanted feature\n",
        "  url = remove_last_number(url)\n",
        "  # Specify a pattern\n",
        "  match = re.search(r'/([^/]+)/([^/]+)/$', url)\n",
        "  # If found extract the city\n",
        "  if match:\n",
        "      city = match.group(2)\n",
        "  # Else set it as null\n",
        "  else:\n",
        "      city = None\n",
        "  return city"
      ],
      "metadata": {
        "id": "VBGDiagEQYsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We again do the same for the category of services"
      ],
      "metadata": {
        "id": "s37YDWMaQsfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category(url):\n",
        "  '''\n",
        "  Given an url extract the category of the services offered\n",
        "  '''\n",
        "  # Remove unwanted feature\n",
        "  url = remove_last_number(url)\n",
        "  # Get the category part of the url\n",
        "  url_parts = url.split('/')\n",
        "  category = url_parts[4]\n",
        "  return category"
      ],
      "metadata": {
        "id": "5qyDISV0Q7zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter the url and extract it's code"
      ],
      "metadata": {
        "id": "U0c6vVuRRO8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soup(url):\n",
        "  '''\n",
        "  Given an url, access it and return its code\n",
        "  '''\n",
        "  # Instantiate driver\n",
        "  driver = webdriver_example()\n",
        "  # Access the url\n",
        "  driver.get(url)\n",
        "  # Get it's page source\n",
        "  page_source = driver.page_source\n",
        "  # Parse it\n",
        "  soup = BeautifulSoup(page_source, 'html.parser')\n",
        "  return soup"
      ],
      "metadata": {
        "id": "ZQOWDMfBRXLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this particular case, our data storage functions will implement csv locking.\n",
        "This is done because we'll use multiprocessing and we'll have several processes writing in the same file, what we'll do is lock that file while is being written so only one process at each time can write in that file. This is done to prevent interferences between processes the downside is it will slow down a little bit our multiprocessing because processes we'll have to wait to have access to the file but it's necessary for data integrity."
      ],
      "metadata": {
        "id": "IJy_m4fsO7Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_csv(data ,province ,city):\n",
        "  # Create a lock\n",
        "  csv_lock = threading.Lock()\n",
        "  # Lock the file\n",
        "  with csv_lock:\n",
        "      # Get today's date time\n",
        "      current_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "      # Get the file path\n",
        "      csv_file = f\"C:\\\\Users\\\\Usuario\\\\PycharmProjects\\\\paginas_amarillas\\\\paginas_amarillas_{current_date}.csv\"\n",
        "\n",
        "      # Check if it already exists.\n",
        "      file_exists = os.path.isfile(csv_file)\n",
        "\n",
        "      # Write if it doesn't exist append else\n",
        "      with open(csv_file, mode='a' if file_exists else 'w', newline='') as file:\n",
        "          writer = csv.writer(file)\n",
        "\n",
        "          if not file_exists:\n",
        "              # Create a header\n",
        "              header = [\"Title\", \"Price\", \"Reduction\", \"Rooms\", \"Bathrooms\", \"Meters\", \"Province\"]\n",
        "              writer.writerow(header)\n",
        "\n",
        "          # Write data\n",
        "          writer.writerows(data)\n",
        "      # Print for traceability.\n",
        "      return print(f'Csv data written successfully {province}{city}')"
      ],
      "metadata": {
        "id": "0o3x1ltXO04a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a common feature in webscraping, sometimes webpages inside a same domain will be structured differenly, that's really bad for us because we'll have to solve it out in a different and more complicated way.\n",
        "\n",
        "Luckily, the number of structures inside the same domain that are different will be limited, for example in this domain, url's can be of two types of structures. So the urls that are showcasing services can have 2 different HTML's structure.\n",
        "\n",
        "We'll check which type of structure is each page and scrap it accordingly.\n",
        "\n",
        "Detect how many different HTML infrastructures are inside your target domain and define a checker of type of structure and a process that parses it accordingly to the given type.\n",
        "\n",
        "As I said, usually there are a limited number of types, it's difficult to find more than 5 types inside a domain. Decide how many types there are, how to scrap each one and if it's worth it.\n",
        "\n",
        "We'll now define a function to check which type of structure is a given url. We will have type 1 and type 2."
      ],
      "metadata": {
        "id": "H-52e4XQR9vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def type_checker(soup):\n",
        "  '''\n",
        "  Check of which type is a given url\n",
        "  '''\n",
        "  # Having this characteristic differentiates which class of structure is a given url, if\n",
        "  # this feature is present then the url is of type 2, else it's of type 1. You'll have to discover this for your target domain by yourself by looking into\n",
        "  # the structure of different url's and checking how many types and what is the key characteristic for each type\n",
        "  map = soup.find('div', class_=\"mapping\")\n",
        "  if map:\n",
        "      type = 2\n",
        "  else:\n",
        "      type = 1\n",
        "  return type"
      ],
      "metadata": {
        "id": "N8ieHNUvTbgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know of which HTML structure is each url, we are going to define a parsing process for each type.\n",
        "\n",
        "Having a different structure means our desired data will be located on different HTML tags, so we have to adapt the parsing process to correctly access the HTML code to retrieve the desired data.\n",
        "\n",
        "Let's start with type 1."
      ],
      "metadata": {
        "id": "7rBSOVsST9SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_one(soup, province, city):\n",
        "  '''\n",
        "  Given a soup object(code of the webpage) of structure type 1, a province and a city extract the desired data of the webpage\n",
        "  '''\n",
        "  # Find the boxes that have the data for each advertisement\n",
        "  boxes = soup.find_all('div', class_=\"box\")\n",
        "  # Create a list to store data\n",
        "  data = []\n",
        "  # For each advertisement\n",
        "  for box in boxes:\n",
        "      # Get the name\n",
        "      name = box.find('span', {'itemprop': 'name'})\n",
        "      # If name does not exist, we do not collect data\n",
        "      if name is not None:\n",
        "          # Get the name\n",
        "          name = name.text\n",
        "          # Get the category of the advertisement, which type of service is it\n",
        "          category_p = box.find('p', class_='categ')\n",
        "          category = category_p.text if category_p else \"-\"\n",
        "          # Get the address\n",
        "          address_span = box.find('span', {'itemprop': 'streetAddress'})\n",
        "          address = address_span.text if address_span else \"-\"\n",
        "          # Get the postal code\n",
        "          postal_code_span = box.find('span', {'itemprop': 'postalCode'})\n",
        "          postal_code = postal_code_span.text if postal_code_span else \"-\"\n",
        "          # Get the telephone\n",
        "          telephone_span = box.find('span', {'itemprop': 'telephone'})\n",
        "          telephone = telephone_span.text if telephone_span else \"-\"\n",
        "          # Get the web.\n",
        "          web_span = box.find('a', class_='web')\n",
        "          web = web_span.get('href') if web_span else \"-\"\n",
        "          # Get the technology with which the web was constructed if it's possible, I'll explain what's the purpose of this\n",
        "          try:\n",
        "              techno = builtwith.builtwith(web) if web != \"-\" else \"-\"\n",
        "          except Exception:\n",
        "              techno = \"Unable to decode\"\n",
        "          # Get the description of the advertisement\n",
        "          description_span = box.find('div', {'itemprop': 'description'})\n",
        "          # If it exists, extract text\n",
        "          if description_span:\n",
        "              p_tag_span = description_span.find('p')\n",
        "              description = p_tag_span.text if p_tag_span else \"-\"\n",
        "          # else, set it as null\n",
        "          else:\n",
        "              description = \"-\"\n",
        "          # Store data\n",
        "          data.append((name, category, address, postal_code, telephone, web, techno, description, province, city))\n",
        "      # If the adverisement has no name don't capture it\n",
        "      else:\n",
        "          pass\n",
        "  # Write the csv with the data\n",
        "  get_csv(data, province, city)\n",
        "  # Print for traceability\n",
        "  return print(f'Se han escrito{len(data)}')"
      ],
      "metadata": {
        "id": "YglEMlNUUbI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type 2"
      ],
      "metadata": {
        "id": "IHp_Lb88PtH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_two(soup ,province ,city ,category):\n",
        "  '''\n",
        "  Given a soup object(code of the webpage) of structure type 2, a province, city and category extract the desired data of the webpage\n",
        "  '''\n",
        "  # Get each advertisement\n",
        "  boxes = soup.find_all('div', class_=\"box\")\n",
        "  # Create a data storage\n",
        "  data = []\n",
        "  # For each advertisement\n",
        "  for box in boxes:\n",
        "    # Get the name\n",
        "    name = box.find('h2', {'itemprop': 'name'})\n",
        "    if name is not None:\n",
        "        name = name.text\n",
        "        # Category\n",
        "        categoria = category\n",
        "        # Address\n",
        "        address_span = box.find('span', {'itemprop': 'streetAddress'})\n",
        "        address = address_span.text if address_span else \"-\"\n",
        "        # Postal code\n",
        "        postal_code_span = box.find('span', {'itemprop': 'postalCode'})\n",
        "        postal_code = postal_code_span.text if postal_code_span else \"-\"\n",
        "        # Phone\n",
        "        telephone_span = box.find('span', {'itemprop': 'telephone'})\n",
        "        telephone = telephone_span.text if telephone_span else \"-\"\n",
        "        # Set web as null, this type of structure has no web\n",
        "        web = '-'\n",
        "        # Set techno as null\n",
        "        techno = '-'\n",
        "        # Set description as null, this type of structure has no description\n",
        "        description = '-'\n",
        "        # Save data\n",
        "        data.append((name, categoria, address, postal_code, telephone, web, techno, description, province, city))\n",
        "    # If no name, don't include a register\n",
        "    else:\n",
        "        pass\n",
        "  # Store the data into a csv\n",
        "  get_csv(data ,province ,city)\n",
        "  # Print for traceability\n",
        "  return print(f'Se han escrito {len(data)} de {categoria}')"
      ],
      "metadata": {
        "id": "X4_qfhZyWynI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one will get us the next link of a catalog given an url"
      ],
      "metadata": {
        "id": "BlZvIrrlfojs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_link(soup):\n",
        "  '''\n",
        "  Get next link of the catalog given the soup object of a url\n",
        "  '''\n",
        "  # Find the container of the next link\n",
        "  pagination_ul = soup.find('ul', class_='pagination')\n",
        "  # If there is one\n",
        "  if pagination_ul:\n",
        "    # Find all the li tags inside\n",
        "      li_tags = pagination_ul.find_all('li')\n",
        "      # If it follows this pattern\n",
        "      if str(li_tags[-1]) == '<li></li>':\n",
        "        # Extract this\n",
        "          last_li = li_tags[-2]\n",
        "      else:\n",
        "        # Extract that\n",
        "          last_li = li_tags[-1]\n",
        "      # Get the anchor tag of the li\n",
        "      last_a = last_li.find('a')\n",
        "      # Get the href value of the a anchor\n",
        "      href_value = last_a.get('href') if last_a else ''\n",
        "      # If it exists\n",
        "      if href_value is not None and href_value != 'javascript:void()':\n",
        "          # Assign the link to a variable\n",
        "          next_link = href_value\n",
        "      else:\n",
        "          # There is no next link\n",
        "          next_link = ''\n",
        "  else:\n",
        "      # There is no next_link\n",
        "      next_link = ''\n",
        "  return next_link"
      ],
      "metadata": {
        "id": "Vo-28oV5fspm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's put it all together.\n",
        "\n",
        "Firstly we'll generate a file with all the url's to scrap using the functions we have defined previously."
      ],
      "metadata": {
        "id": "8U-5Bz4igWnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the links for each city/town/village\n",
        "localidades = get_links_provincia('https://www.paginasamarillas.es/all_tarragona_reus.html')\n",
        "total_links = []\n",
        "# For each city/town/village\n",
        "for i in localidades:\n",
        "    # Get the links of all the services offered in that given city/town/village\n",
        "    sectores = get_links_localidad(i)\n",
        "    # Save the data\n",
        "    total_links.append(sectores)\n",
        "# Convert it into a single list, original format is list of lists.\n",
        "total_links = [item for sublist in total_links for item in sublist]\n",
        "# Write the file with all the url's we have to scrap.\n",
        "with open('urls.txt', 'w') as file:\n",
        "    for link in total_links:\n",
        "        file.write(link + '\\n')"
      ],
      "metadata": {
        "id": "58uG9lq3gnOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the urls let's scrap each one. We'll define a function describing what we will do for each url, this function will use all the utility and process functions defined above"
      ],
      "metadata": {
        "id": "VQfOwp5shFPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def micro_url(url):\n",
        "  '''\n",
        "  Given a url, scrap all the data of it and the next pages of the catalog in the same tree url\n",
        "  '''\n",
        "  # Get the province to which the url belongs\n",
        "  province = get_province(url)\n",
        "  # Get the city to which the url belongs\n",
        "  city = get_city(url)\n",
        "  # Get the category of services to which the url belongs\n",
        "  category = get_category(url)\n",
        "  # Get the raw code of the url\n",
        "  soup = get_soup(url)\n",
        "  # Check which type of HTML structure the url is\n",
        "  type = type_checker(soup)\n",
        "  # If its type one\n",
        "  if type == 1:\n",
        "      # Parse accordingly to type 1\n",
        "      parse_one(soup ,province ,city)\n",
        "      # Get next link of the catalog\n",
        "      next_link = get_next_link(soup)\n",
        "  # If its type two\n",
        "  else:\n",
        "      # Parse accordingly to type 2\n",
        "      parse_two(soup ,province ,city, category)\n",
        "      # There is no next_link for type 2, set it as null\n",
        "      next_link = ''\n",
        "  # If there is a next_link\n",
        "  while next_link != '':\n",
        "      print(\"I'm in next_link\")\n",
        "      # Get the province of next_link\n",
        "      province = get_province(next_link)\n",
        "      # Get the city of next_link\n",
        "      city = get_city(next_link)\n",
        "      # Get the category of next_link\n",
        "      category = get_category(next_link)\n",
        "      # Get the raw code of next link\n",
        "      soup = get_soup(next_link)\n",
        "      print(f'Scrapped {next_link}')\n",
        "      # Check for HTML structure type\n",
        "      type = type_checker(soup)\n",
        "      # If type 1\n",
        "      if type == 1:\n",
        "          # Parse the code accordingly\n",
        "          parse_one(soup, province, city)\n",
        "          # Get next_link\n",
        "          next_link = get_next_link(soup)\n",
        "      # If type 2\n",
        "      else:\n",
        "          # Parse type two\n",
        "          parse_two(soup, province, city ,category)\n",
        "          # Set null next_link, type two has no next link\n",
        "          next_link = ''\n",
        "      print(f'Getting {next_link} 1')\n",
        "  # When reaching here means we have already scraped all the url tree of the original url provided to the function, so means there is no next link in the catalog to scrap for that url.\n",
        "  # Print for traceability\n",
        "  print('Scapped the while loop 1')\n",
        "  print(url, \"Scraped succesfully\")"
      ],
      "metadata": {
        "id": "4-q5xDVagY_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already have defined what to do with each url of the generated file.\n",
        "Now let's put it all together in a main process and let's use multiprocessing to scrap multiple url's of the file at the same time so we speed up the process."
      ],
      "metadata": {
        "id": "rWnYREHbivb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  '''\n",
        "  Main process of the use-case\n",
        "  '''\n",
        "  # Open the generated file, change the route to yours\n",
        "  with open('C:\\\\Users\\\\Usuario\\\\PycharmProjects\\\\paginas_amarillas\\\\urls.txt', 'r') as file:\n",
        "      # Parse each line of it and get a list of urls to scrap\n",
        "      url_list = [line.strip() for line in file]\n",
        "\n",
        "  # Set the maximum number of parallel processes, usually leave 1 or 2 CPU's for other purposes\n",
        "  max_processes = 10\n",
        "  # Set up a pool of processes\n",
        "  with concurrent.futures.ProcessPoolExecutor(max_processes) as executor:\n",
        "      # Each process executes the micro_url function to one url until there are no more in the list\n",
        "      futures = [executor.submit(micro_url, url) for url in url_list]\n",
        "      for future in concurrent.futures.as_completed(futures):\n",
        "          # Process has completed\n",
        "          print(f\"Number of completed processes: {len([f for f in futures if f.done()])}\")\n",
        "  # Wait for all processes to finish\n",
        "  concurrent.futures.wait(futures, return_when=concurrent.futures.ALL_COMPLETED)\n",
        "\n",
        "# Call the process\n",
        "if __name__ == \"__main__\":\n",
        "    # Time it\n",
        "    initial = time.time()\n",
        "    # Execute\n",
        "    main()\n",
        "    # Time finish time\n",
        "    end_time = time.time()\n",
        "    # Get and print total time of process\n",
        "    elapsed_time = end_time - initial\n",
        "    print(f\"Processed in {elapsed_time} seconds.\")"
      ],
      "metadata": {
        "id": "ekO2NxlEi6Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Result and conclusion of use-case**"
      ],
      "metadata": {
        "id": "jwGgPunGj2Ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting csv can be found here:\n",
        "\n",
        "https://raw.githubusercontent.com/zetag33/WebScraping-Tutorial-Use-Case/main/dataset/paginas_amarillas_2023_11_16.csv\n",
        "Data about 110.000 businesses.\n",
        "\n",
        "The techno feature we are extracting, gets with which technology the website of the business was built. This was done to be able to have data about businesses that could need to build a website if they had none or improve an existing one if it was built with an old technology.\n",
        "\n",
        "So by extracting this data we are able to built a customer base for a website offering business.\n",
        "\n",
        "Proxies have not been used since the server doesn't cut off the process and by faking the user-agent is enough."
      ],
      "metadata": {
        "id": "tnQidEN0j8j8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "Yda-HZxnqkhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Webscraping is a diverse business.\n",
        "This tutorial/guide is built with the intention to provide you with examples of real world applications.\n",
        "It's also built to provide the basic code structure and problem approach methodology to most webscraping use-cases.\n",
        "We have learnt how to scrap a catalog based website, having the website a page for a catalog itself or by displaying it in some url of the domain.\n",
        "We also have learnt how to search for a specific product and retrieve the data we are interested in, it could be by endless scrolling or by again passing pages of a catalog.\n",
        "We have learnt how to multiprocess it in order to speed up.\n",
        "How to store data in a csv and how to deal with multiprocesses writing a same csv.\n",
        "\n",
        "The use case might change and as explained in the tutorial you'll need to adapt some aspects of the code to your target domain, such as location of HTML tags and intrisic aspects of the infrastructure of the code.\n",
        "I have provided examples of how to use so.\n",
        "I would advise to use this tutorial as a guideline and to get functions and methodology that will be useful for your particular project.\n",
        "\n",
        "Webscraping is a business that requires thinking out of the box, experimentation and troubleshooting problems of all kinds. You'll also have to deal with diverse firewall problems, the troubleshooting of firewall done in this code is basic but effective, for greater projects you'll probably have to handle CAPTCHA's and  more effective protection tools. This could be another whole topic itself.\n",
        "\n",
        "As a wise man once said, I did not fish for you but I taught you the tools to do so."
      ],
      "metadata": {
        "id": "w8vAhhHHqns8"
      }
    }
  ]
}